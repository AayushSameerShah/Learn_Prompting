"use strict";(self.webpackChunkpromptgineering=self.webpackChunkpromptgineering||[]).push([[170],{3905:(e,a,t)=>{t.d(a,{Zo:()=>f,kt:()=>d});var n=t(7294);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function i(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=n.createContext({}),p=function(e){var a=n.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):i(i({},a),e)),t},f=function(e){var a=p(e.components);return n.createElement(l.Provider,{value:a},e.children)},m={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},h=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,f=s(e,["components","mdxType","originalType","parentName"]),h=p(t),d=r,u=h["".concat(l,".").concat(d)]||h[d]||m[d]||o;return t?n.createElement(u,i(i({ref:a},f),{},{components:t})):n.createElement(u,i({ref:a},f))}));function d(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var o=t.length,i=new Array(o);i[0]=h;var s={};for(var l in a)hasOwnProperty.call(a,l)&&(s[l]=a[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var p=2;p<o;p++)i[p]=t[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,t)}h.displayName="MDXCreateElement"},8363:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var n=t(7462),r=(t(7294),t(3905));const o={sidebar_position:1e3},i="\ud83d\udcda Bibliography",s={unversionedId:"bibliography",id:"bibliography",title:"\ud83d\udcda Bibliography",description:"The page contains an organized list of all papers used by this course.",source:"@site/docs/bibliography.md",sourceDirName:".",slug:"/bibliography",permalink:"/es/docs/bibliography",draft:!1,editUrl:"https://github.com/trigaten/promptgineering/tree/v0.0.2/docs/bibliography.md",tags:[],version:"current",sidebarPosition:1e3,frontMatter:{sidebar_position:1e3},sidebar:"tutorialSidebar",previous:{title:"\ud83d\udcd9 Vocabulary Reference",permalink:"/es/docs/vocabulary"},next:{title:"\u2728 Credits",permalink:"/es/docs/credits"}},l={},p=[{value:"Prompt Engineering Strategies",id:"prompt-engineering-strategies",level:2},{value:"Chain of Thought(@wei2022chain) \ud83d\udd35",id:"chain-of-thoughtwei2022chain-",level:4},{value:"Zero Shot Chain of Thought(@kojima2022large) \ud83d\udd35",id:"zero-shot-chain-of-thoughtkojima2022large-",level:4},{value:"Self Consistency(@wang2022selfconsistency) \ud83d\udd35",id:"self-consistencywang2022selfconsistency-",level:4},{value:"What Makes Good In-Context Examples for GPT-3?(@liu2021makes) \ud83d\udd35",id:"what-makes-good-in-context-examples-for-gpt-3liu2021makes-",level:4},{value:"Generated Knowledge(@liu2021generated) \ud83d\udd35",id:"generated-knowledgeliu2021generated-",level:4},{value:"Rethinking the role of demonstrations(@min2022rethinking) \ud83d\udd35",id:"rethinking-the-role-of-demonstrationsmin2022rethinking-",level:4},{value:"Scratchpads(@nye2021work)",id:"scratchpadsnye2021work",level:4},{value:"Maieutic Prompting(@jung2022maieutic)",id:"maieutic-promptingjung2022maieutic",level:4},{value:"STaR(@zelikman2022star)",id:"starzelikman2022star",level:4},{value:"Least to Most(@zhou2022leasttomost)",id:"least-to-mostzhou2022leasttomost",level:4},{value:"Reliability",id:"reliability",level:2},{value:"The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning(@ye2022unreliability) \ud83d\udd35",id:"the-unreliability-of-explanations-in-few-shot-prompting-for-textual-reasoningye2022unreliability-",level:4},{value:"Prompting GPT-3 to be reliable(@si2022prompting)",id:"prompting-gpt-3-to-be-reliablesi2022prompting",level:4},{value:"Diverse Prompts(@li2022advance) \ud83d\udd35",id:"diverse-promptsli2022advance-",level:4},{value:"Calibrate Before Use: Improving Few-Shot Performance of Language Models(@zhao2021calibrate) \ud83d\udd35",id:"calibrate-before-use-improving-few-shot-performance-of-language-modelszhao2021calibrate-",level:4},{value:"Enhanced Self Consistency(@mitchell2022enhancing)",id:"enhanced-self-consistencymitchell2022enhancing",level:4},{value:"Automated Prompt Engineering",id:"automated-prompt-engineering",level:2},{value:"AutoPrompt(@shin2020autoprompt) \ud83d\udd35",id:"autopromptshin2020autoprompt-",level:4},{value:"Automatic Prompt Engineer(@zhou2022large)",id:"automatic-prompt-engineerzhou2022large",level:4},{value:"Models",id:"models",level:2},{value:"Language Models",id:"language-models",level:3},{value:"GPT-3(@brown2020language) \ud83d\udd35",id:"gpt-3brown2020language-",level:4},{value:"GPT-3 Instruct(@ouyang2022training) \ud83d\udd35",id:"gpt-3-instructouyang2022training-",level:4},{value:"PaLM(@chowdhery2022palm) \ud83d\udd35",id:"palmchowdhery2022palm-",level:4},{value:"BLOOM(@scao2022bloom) \ud83d\udd35",id:"bloomscao2022bloom-",level:4},{value:"BLOOM+1 (more languages/ 0 shot improvements)(@yong2022bloom1)",id:"bloom1-more-languages-0-shot-improvementsyong2022bloom1",level:4},{value:"Jurassic 1(@lieberjurassic) \ud83d\udd35",id:"jurassic-1lieberjurassic-",level:4},{value:"GPT-J-6B(@wange2021gptj)",id:"gpt-j-6bwange2021gptj",level:4},{value:"Roberta(@liu2019roberta)",id:"robertaliu2019roberta",level:4},{value:"Image Models",id:"image-models",level:3},{value:"Stable Diffusion(@rombach2021highresolution) \ud83d\udd35",id:"stable-diffusionrombach2021highresolution-",level:4},{value:"DALLE(@ramesh2022hierarchical) \ud83d\udd35",id:"dalleramesh2022hierarchical-",level:4},{value:"Soft Prompting",id:"soft-prompting",level:2},{value:"Soft Prompting(@lester2021power) \ud83d\udd35",id:"soft-promptinglester2021power-",level:4},{value:"Interpretable Discretized Soft Prompts(@khashabi2021prompt) \ud83d\udd35",id:"interpretable-discretized-soft-promptskhashabi2021prompt-",level:4},{value:"Datasets",id:"datasets",level:2},{value:"GSM8K(@cobbe2021training) \ud83d\udd35",id:"gsm8kcobbe2021training-",level:4},{value:"HotPotQA(@yang2018hotpotqa) \ud83d\udd35",id:"hotpotqayang2018hotpotqa-",level:4},{value:"Fever(@thorne2018fever) \ud83d\udd35",id:"feverthorne2018fever-",level:4},{value:"BBQ: A Hand-Built Bias Benchmark for Question Answering(@parrish2021bbq) \ud83d\udd35",id:"bbq-a-hand-built-bias-benchmark-for-question-answeringparrish2021bbq-",level:4},{value:"Image Prompt Engineering",id:"image-prompt-engineering",level:2},{value:"Taxonomy of prompt modifiers(@oppenlaender2022taxonomy)",id:"taxonomy-of-prompt-modifiersoppenlaender2022taxonomy",level:4},{value:"DiffusionDB(@wang2022diffusiondb)",id:"diffusiondbwang2022diffusiondb",level:4},{value:"The DALLE 2 Prompt Book(@parsons2022dalleprompt) \ud83d\udd35",id:"the-dalle-2-prompt-bookparsons2022dalleprompt-",level:4},{value:"Prompt Engineering for Text-Based Generative Art(@oppenlaender2022prompt)",id:"prompt-engineering-for-text-based-generative-artoppenlaender2022prompt",level:4},{value:"Prompt Engineering IDEs",id:"prompt-engineering-ides",level:2},{value:"Prompt IDE(@strobelt2022promptide) \ud83d\udd35",id:"prompt-idestrobelt2022promptide-",level:4},{value:"Prompt Source(@bach2022promptsource) \ud83d\udd35",id:"prompt-sourcebach2022promptsource-",level:4},{value:"PromptChainer(@wu2022promptchainer) \ud83d\udd35",id:"promptchainerwu2022promptchainer-",level:4},{value:"OpenPrompt: An Open-source Framework for Prompt-learning(@ding2021openprompt) \ud83d\udd35",id:"openprompt-an-open-source-framework-for-prompt-learningding2021openprompt-",level:4},{value:"PromptMaker(@jiang2022promptmaker) \ud83d\udd35",id:"promptmakerjiang2022promptmaker-",level:4},{value:"Applied Prompt Engineering",id:"applied-prompt-engineering",level:2},{value:"Language Model Cascades(@dohan2022language)",id:"language-model-cascadesdohan2022language",level:4},{value:"MRKL(@karpas2022mrkl) \ud83d\udd35",id:"mrklkarpas2022mrkl-",level:4},{value:"ReAct(@yao2022react) \ud83d\udd35",id:"reactyao2022react-",level:4},{value:"PAL: Program-aided Language Models(@gao2022pal) \ud83d\udd35",id:"pal-program-aided-language-modelsgao2022pal-",level:4},{value:"User Interface Design",id:"user-interface-design",level:2},{value:"Design Guidelines for Prompt Engineering Text-to-Image Generative Models(@liu2022design)",id:"design-guidelines-for-prompt-engineering-text-to-image-generative-modelsliu2022design",level:4},{value:"Prompt Injection",id:"prompt-injection",level:2},{value:"Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods(@crothers2022machine) \ud83d\udd35",id:"machine-generated-text-a-comprehensive-survey-of-threat-models-and-detection-methodscrothers2022machine-",level:4},{value:"Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples(@branch2022evaluating) \ud83d\udd35",id:"evaluating-the-susceptibility-of-pre-trained-language-models-via-handcrafted-adversarial-examplesbranch2022evaluating-",level:4},{value:"Prompt injection attacks against GPT-3(@simon2022inject) \ud83d\udd35",id:"prompt-injection-attacks-against-gpt-3simon2022inject-",level:4},{value:"Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions(@goodside2022inject) \ud83d\udd35",id:"exploiting-gpt-3-prompts-with-malicious-inputs-that-order-the-model-to-ignore-its-previous-directionsgoodside2022inject-",level:4},{value:"adversarial-prompts(@chase2021adversarial) \ud83d\udd35",id:"adversarial-promptschase2021adversarial-",level:4},{value:"GPT-3 Prompt Injection Defenses(@goodside2021gpt) \ud83d\udd35",id:"gpt-3-prompt-injection-defensesgoodside2021gpt-",level:4},{value:"Talking to machines: prompt engineering &amp; injection(@christoph2022talking)",id:"talking-to-machines-prompt-engineering--injectionchristoph2022talking",level:4},{value:"Surveys",id:"surveys",level:2},{value:"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing(@liu2021pretrain)",id:"pre-train-prompt-and-predict-a-systematic-survey-of-prompting-methods-in-natural-language-processingliu2021pretrain",level:4},{value:"PromptPapers(@ning2022papers)",id:"promptpapersning2022papers",level:4},{value:"Miscl",id:"miscl",level:2},{value:"Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models(@bursztyn2022learning)",id:"learning-to-perform-complex-tasks-through-compositional-fine-tuning-of-language-modelsbursztyn2022learning",level:4},{value:"Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks(@wang2022supernaturalinstructions)",id:"super-naturalinstructions-generalization-via-declarative-instructions-on-1600-nlp-taskswang2022supernaturalinstructions",level:4},{value:"Making Pre-trained Language Models Better Few-shot Learners(@gao2021making)",id:"making-pre-trained-language-models-better-few-shot-learnersgao2021making",level:4},{value:"Grounding with search results(@livin2022large)",id:"grounding-with-search-resultslivin2022large",level:4},{value:"How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models(@dang2022prompt)",id:"how-to-prompt-opportunities-and-challenges-of-zero--and-few-shot-learning-for-human-ai-interaction-in-creative-applications-of-generative-modelsdang2022prompt",level:4},{value:"On Measuring Social Biases in Prompt-Based Multi-Task Learning(@akyrek2022measuring)",id:"on-measuring-social-biases-in-prompt-based-multi-task-learningakyrek2022measuring",level:4},{value:"Plot Writing From Pre-Trained Language Models(@jin2022plot) \ud83d\udd35",id:"plot-writing-from-pre-trained-language-modelsjin2022plot-",level:4},{value:"Examples(@2022examples)",id:"examples2022examples",level:4},{value:"Wordcraft(@yuan2022wordcraft)",id:"wordcraftyuan2022wordcraft",level:4},{value:"PainPoints(@fadnavis2022pain)",id:"painpointsfadnavis2022pain",level:4},{value:"Self-Instruct: Aligning Language Model with Self Generated Instructions(@wang2022selfinstruct)",id:"self-instruct-aligning-language-model-with-self-generated-instructionswang2022selfinstruct",level:4},{value:"From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models(@guo2022images)",id:"from-images-to-textual-prompts-zero-shot-vqa-with-frozen-large-language-modelsguo2022images",level:4}],f={toc:p};function m(e){let{components:a,...t}=e;return(0,r.kt)("wrapper",(0,n.Z)({},f,t,{components:a,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"-bibliography"},"\ud83d\udcda Bibliography"),(0,r.kt)("p",null,"The page contains an organized list of all papers used by this course.\nThe papers are organized by topic."),(0,r.kt)("p",null,"\ud83d\udd35 = Paper directly cited in this course. Other papers have informed my understanding of the topic."),(0,r.kt)("p",null,"Note: since ",(0,r.kt)("a",{parentName:"p",href:"https://twitter.com/janleike/status/1584618242756132864"},"neither the GPT-3 nor the GPT-3 Instruct paper correspond to davinci models"),", I attempt not to\ncite them as such."),(0,r.kt)("h2",{id:"prompt-engineering-strategies"},"Prompt Engineering Strategies"),(0,r.kt)("h4",{id:"chain-of-thoughtwei2022chain-"},"Chain of Thought",(0,r.kt)("sup",{parentName:"h4",id:"fnref-1"},(0,r.kt)("a",{parentName:"sup",href:"#fn-1",className:"footnote-ref"},"1"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"zero-shot-chain-of-thoughtkojima2022large-"},"Zero Shot Chain of Thought",(0,r.kt)("sup",{parentName:"h4",id:"fnref-2"},(0,r.kt)("a",{parentName:"sup",href:"#fn-2",className:"footnote-ref"},"2"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"self-consistencywang2022selfconsistency-"},"Self Consistency",(0,r.kt)("sup",{parentName:"h4",id:"fnref-3"},(0,r.kt)("a",{parentName:"sup",href:"#fn-3",className:"footnote-ref"},"3"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"what-makes-good-in-context-examples-for-gpt-3liu2021makes-"},"What Makes Good In-Context Examples for GPT-3?",(0,r.kt)("sup",{parentName:"h4",id:"fnref-4"},(0,r.kt)("a",{parentName:"sup",href:"#fn-4",className:"footnote-ref"},"4"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"generated-knowledgeliu2021generated-"},"Generated Knowledge",(0,r.kt)("sup",{parentName:"h4",id:"fnref-5"},(0,r.kt)("a",{parentName:"sup",href:"#fn-5",className:"footnote-ref"},"5"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"rethinking-the-role-of-demonstrationsmin2022rethinking-"},"Rethinking the role of demonstrations",(0,r.kt)("sup",{parentName:"h4",id:"fnref-6"},(0,r.kt)("a",{parentName:"sup",href:"#fn-6",className:"footnote-ref"},"6"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"scratchpadsnye2021work"},"Scratchpads",(0,r.kt)("sup",{parentName:"h4",id:"fnref-7"},(0,r.kt)("a",{parentName:"sup",href:"#fn-7",className:"footnote-ref"},"7"))),(0,r.kt)("h4",{id:"maieutic-promptingjung2022maieutic"},"Maieutic Prompting",(0,r.kt)("sup",{parentName:"h4",id:"fnref-8"},(0,r.kt)("a",{parentName:"sup",href:"#fn-8",className:"footnote-ref"},"8"))),(0,r.kt)("h4",{id:"starzelikman2022star"},"STaR",(0,r.kt)("sup",{parentName:"h4",id:"fnref-9"},(0,r.kt)("a",{parentName:"sup",href:"#fn-9",className:"footnote-ref"},"9"))),(0,r.kt)("h4",{id:"least-to-mostzhou2022leasttomost"},"Least to Most",(0,r.kt)("sup",{parentName:"h4",id:"fnref-10"},(0,r.kt)("a",{parentName:"sup",href:"#fn-10",className:"footnote-ref"},"10"))),(0,r.kt)("h2",{id:"reliability"},"Reliability"),(0,r.kt)("h4",{id:"the-unreliability-of-explanations-in-few-shot-prompting-for-textual-reasoningye2022unreliability-"},"The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",(0,r.kt)("sup",{parentName:"h4",id:"fnref-11"},(0,r.kt)("a",{parentName:"sup",href:"#fn-11",className:"footnote-ref"},"11"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"prompting-gpt-3-to-be-reliablesi2022prompting"},"Prompting GPT-3 to be reliable",(0,r.kt)("sup",{parentName:"h4",id:"fnref-12"},(0,r.kt)("a",{parentName:"sup",href:"#fn-12",className:"footnote-ref"},"12"))),(0,r.kt)("h4",{id:"diverse-promptsli2022advance-"},"Diverse Prompts",(0,r.kt)("sup",{parentName:"h4",id:"fnref-13"},(0,r.kt)("a",{parentName:"sup",href:"#fn-13",className:"footnote-ref"},"13"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"calibrate-before-use-improving-few-shot-performance-of-language-modelszhao2021calibrate-"},"Calibrate Before Use: Improving Few-Shot Performance of Language Models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-14"},(0,r.kt)("a",{parentName:"sup",href:"#fn-14",className:"footnote-ref"},"14"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"enhanced-self-consistencymitchell2022enhancing"},"Enhanced Self Consistency",(0,r.kt)("sup",{parentName:"h4",id:"fnref-15"},(0,r.kt)("a",{parentName:"sup",href:"#fn-15",className:"footnote-ref"},"15"))),(0,r.kt)("h2",{id:"automated-prompt-engineering"},"Automated Prompt Engineering"),(0,r.kt)("h4",{id:"autopromptshin2020autoprompt-"},"AutoPrompt",(0,r.kt)("sup",{parentName:"h4",id:"fnref-16"},(0,r.kt)("a",{parentName:"sup",href:"#fn-16",className:"footnote-ref"},"16"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"automatic-prompt-engineerzhou2022large"},"Automatic Prompt Engineer",(0,r.kt)("sup",{parentName:"h4",id:"fnref-17"},(0,r.kt)("a",{parentName:"sup",href:"#fn-17",className:"footnote-ref"},"17"))),(0,r.kt)("h2",{id:"models"},"Models"),(0,r.kt)("h3",{id:"language-models"},"Language Models"),(0,r.kt)("h4",{id:"gpt-3brown2020language-"},"GPT-3",(0,r.kt)("sup",{parentName:"h4",id:"fnref-18"},(0,r.kt)("a",{parentName:"sup",href:"#fn-18",className:"footnote-ref"},"18"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"gpt-3-instructouyang2022training-"},"GPT-3 Instruct",(0,r.kt)("sup",{parentName:"h4",id:"fnref-19"},(0,r.kt)("a",{parentName:"sup",href:"#fn-19",className:"footnote-ref"},"19"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"palmchowdhery2022palm-"},"PaLM",(0,r.kt)("sup",{parentName:"h4",id:"fnref-20"},(0,r.kt)("a",{parentName:"sup",href:"#fn-20",className:"footnote-ref"},"20"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"bloomscao2022bloom-"},"BLOOM",(0,r.kt)("sup",{parentName:"h4",id:"fnref-21"},(0,r.kt)("a",{parentName:"sup",href:"#fn-21",className:"footnote-ref"},"21"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"bloom1-more-languages-0-shot-improvementsyong2022bloom1"},"BLOOM+1 (more languages/ 0 shot improvements)",(0,r.kt)("sup",{parentName:"h4",id:"fnref-22"},(0,r.kt)("a",{parentName:"sup",href:"#fn-22",className:"footnote-ref"},"22"))),(0,r.kt)("h4",{id:"jurassic-1lieberjurassic-"},"Jurassic 1",(0,r.kt)("sup",{parentName:"h4",id:"fnref-23"},(0,r.kt)("a",{parentName:"sup",href:"#fn-23",className:"footnote-ref"},"23"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"gpt-j-6bwange2021gptj"},"GPT-J-6B",(0,r.kt)("sup",{parentName:"h4",id:"fnref-24"},(0,r.kt)("a",{parentName:"sup",href:"#fn-24",className:"footnote-ref"},"24"))),(0,r.kt)("h4",{id:"robertaliu2019roberta"},"Roberta",(0,r.kt)("sup",{parentName:"h4",id:"fnref-25"},(0,r.kt)("a",{parentName:"sup",href:"#fn-25",className:"footnote-ref"},"25"))),(0,r.kt)("h3",{id:"image-models"},"Image Models"),(0,r.kt)("h4",{id:"stable-diffusionrombach2021highresolution-"},"Stable Diffusion",(0,r.kt)("sup",{parentName:"h4",id:"fnref-26"},(0,r.kt)("a",{parentName:"sup",href:"#fn-26",className:"footnote-ref"},"26"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"dalleramesh2022hierarchical-"},"DALLE",(0,r.kt)("sup",{parentName:"h4",id:"fnref-27"},(0,r.kt)("a",{parentName:"sup",href:"#fn-27",className:"footnote-ref"},"27"))," \ud83d\udd35"),(0,r.kt)("h2",{id:"soft-prompting"},"Soft Prompting"),(0,r.kt)("h4",{id:"soft-promptinglester2021power-"},"Soft Prompting",(0,r.kt)("sup",{parentName:"h4",id:"fnref-28"},(0,r.kt)("a",{parentName:"sup",href:"#fn-28",className:"footnote-ref"},"28"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"interpretable-discretized-soft-promptskhashabi2021prompt-"},"Interpretable Discretized Soft Prompts",(0,r.kt)("sup",{parentName:"h4",id:"fnref-29"},(0,r.kt)("a",{parentName:"sup",href:"#fn-29",className:"footnote-ref"},"29"))," \ud83d\udd35"),(0,r.kt)("h2",{id:"datasets"},"Datasets"),(0,r.kt)("h4",{id:"gsm8kcobbe2021training-"},"GSM8K",(0,r.kt)("sup",{parentName:"h4",id:"fnref-30"},(0,r.kt)("a",{parentName:"sup",href:"#fn-30",className:"footnote-ref"},"30"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"hotpotqayang2018hotpotqa-"},"HotPotQA",(0,r.kt)("sup",{parentName:"h4",id:"fnref-31"},(0,r.kt)("a",{parentName:"sup",href:"#fn-31",className:"footnote-ref"},"31"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"feverthorne2018fever-"},"Fever",(0,r.kt)("sup",{parentName:"h4",id:"fnref-32"},(0,r.kt)("a",{parentName:"sup",href:"#fn-32",className:"footnote-ref"},"32"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"bbq-a-hand-built-bias-benchmark-for-question-answeringparrish2021bbq-"},"BBQ: A Hand-Built Bias Benchmark for Question Answering",(0,r.kt)("sup",{parentName:"h4",id:"fnref-33"},(0,r.kt)("a",{parentName:"sup",href:"#fn-33",className:"footnote-ref"},"33"))," \ud83d\udd35"),(0,r.kt)("h2",{id:"image-prompt-engineering"},"Image Prompt Engineering"),(0,r.kt)("h4",{id:"taxonomy-of-prompt-modifiersoppenlaender2022taxonomy"},"Taxonomy of prompt modifiers",(0,r.kt)("sup",{parentName:"h4",id:"fnref-34"},(0,r.kt)("a",{parentName:"sup",href:"#fn-34",className:"footnote-ref"},"34"))),(0,r.kt)("h4",{id:"diffusiondbwang2022diffusiondb"},"DiffusionDB",(0,r.kt)("sup",{parentName:"h4",id:"fnref-35"},(0,r.kt)("a",{parentName:"sup",href:"#fn-35",className:"footnote-ref"},"35"))),(0,r.kt)("h4",{id:"the-dalle-2-prompt-bookparsons2022dalleprompt-"},"The DALLE 2 Prompt Book",(0,r.kt)("sup",{parentName:"h4",id:"fnref-36"},(0,r.kt)("a",{parentName:"sup",href:"#fn-36",className:"footnote-ref"},"36"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"prompt-engineering-for-text-based-generative-artoppenlaender2022prompt"},"Prompt Engineering for Text-Based Generative Art",(0,r.kt)("sup",{parentName:"h4",id:"fnref-37"},(0,r.kt)("a",{parentName:"sup",href:"#fn-37",className:"footnote-ref"},"37"))),(0,r.kt)("h2",{id:"prompt-engineering-ides"},"Prompt Engineering IDEs"),(0,r.kt)("h4",{id:"prompt-idestrobelt2022promptide-"},"Prompt IDE",(0,r.kt)("sup",{parentName:"h4",id:"fnref-38"},(0,r.kt)("a",{parentName:"sup",href:"#fn-38",className:"footnote-ref"},"38"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"prompt-sourcebach2022promptsource-"},"Prompt Source",(0,r.kt)("sup",{parentName:"h4",id:"fnref-39"},(0,r.kt)("a",{parentName:"sup",href:"#fn-39",className:"footnote-ref"},"39"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"promptchainerwu2022promptchainer-"},"PromptChainer",(0,r.kt)("sup",{parentName:"h4",id:"fnref-40"},(0,r.kt)("a",{parentName:"sup",href:"#fn-40",className:"footnote-ref"},"40"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"openprompt-an-open-source-framework-for-prompt-learningding2021openprompt-"},"OpenPrompt: An Open-source Framework for Prompt-learning",(0,r.kt)("sup",{parentName:"h4",id:"fnref-41"},(0,r.kt)("a",{parentName:"sup",href:"#fn-41",className:"footnote-ref"},"41"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"promptmakerjiang2022promptmaker-"},"PromptMaker",(0,r.kt)("sup",{parentName:"h4",id:"fnref-42"},(0,r.kt)("a",{parentName:"sup",href:"#fn-42",className:"footnote-ref"},"42"))," \ud83d\udd35"),(0,r.kt)("h2",{id:"applied-prompt-engineering"},"Applied Prompt Engineering"),(0,r.kt)("h4",{id:"language-model-cascadesdohan2022language"},"Language Model Cascades",(0,r.kt)("sup",{parentName:"h4",id:"fnref-43"},(0,r.kt)("a",{parentName:"sup",href:"#fn-43",className:"footnote-ref"},"43"))),(0,r.kt)("h4",{id:"mrklkarpas2022mrkl-"},"MRKL",(0,r.kt)("sup",{parentName:"h4",id:"fnref-44"},(0,r.kt)("a",{parentName:"sup",href:"#fn-44",className:"footnote-ref"},"44"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"reactyao2022react-"},"ReAct",(0,r.kt)("sup",{parentName:"h4",id:"fnref-45"},(0,r.kt)("a",{parentName:"sup",href:"#fn-45",className:"footnote-ref"},"45"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"pal-program-aided-language-modelsgao2022pal-"},"PAL: Program-aided Language Models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-46"},(0,r.kt)("a",{parentName:"sup",href:"#fn-46",className:"footnote-ref"},"46"))," \ud83d\udd35"),(0,r.kt)("h2",{id:"user-interface-design"},"User Interface Design"),(0,r.kt)("h4",{id:"design-guidelines-for-prompt-engineering-text-to-image-generative-modelsliu2022design"},"Design Guidelines for Prompt Engineering Text-to-Image Generative Models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-47"},(0,r.kt)("a",{parentName:"sup",href:"#fn-47",className:"footnote-ref"},"47"))),(0,r.kt)("h2",{id:"prompt-injection"},"Prompt Injection"),(0,r.kt)("h4",{id:"machine-generated-text-a-comprehensive-survey-of-threat-models-and-detection-methodscrothers2022machine-"},"Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods",(0,r.kt)("sup",{parentName:"h4",id:"fnref-48"},(0,r.kt)("a",{parentName:"sup",href:"#fn-48",className:"footnote-ref"},"48"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"evaluating-the-susceptibility-of-pre-trained-language-models-via-handcrafted-adversarial-examplesbranch2022evaluating-"},"Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples",(0,r.kt)("sup",{parentName:"h4",id:"fnref-49"},(0,r.kt)("a",{parentName:"sup",href:"#fn-49",className:"footnote-ref"},"49"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"prompt-injection-attacks-against-gpt-3simon2022inject-"},"Prompt injection attacks against GPT-3",(0,r.kt)("sup",{parentName:"h4",id:"fnref-50"},(0,r.kt)("a",{parentName:"sup",href:"#fn-50",className:"footnote-ref"},"50"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"exploiting-gpt-3-prompts-with-malicious-inputs-that-order-the-model-to-ignore-its-previous-directionsgoodside2022inject-"},"Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions",(0,r.kt)("sup",{parentName:"h4",id:"fnref-51"},(0,r.kt)("a",{parentName:"sup",href:"#fn-51",className:"footnote-ref"},"51"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"adversarial-promptschase2021adversarial-"},"adversarial-prompts",(0,r.kt)("sup",{parentName:"h4",id:"fnref-52"},(0,r.kt)("a",{parentName:"sup",href:"#fn-52",className:"footnote-ref"},"52"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"gpt-3-prompt-injection-defensesgoodside2021gpt-"},"GPT-3 Prompt Injection Defenses",(0,r.kt)("sup",{parentName:"h4",id:"fnref-53"},(0,r.kt)("a",{parentName:"sup",href:"#fn-53",className:"footnote-ref"},"53"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"talking-to-machines-prompt-engineering--injectionchristoph2022talking"},"Talking to machines: prompt engineering & injection",(0,r.kt)("sup",{parentName:"h4",id:"fnref-54"},(0,r.kt)("a",{parentName:"sup",href:"#fn-54",className:"footnote-ref"},"54"))),(0,r.kt)("h2",{id:"surveys"},"Surveys"),(0,r.kt)("h4",{id:"pre-train-prompt-and-predict-a-systematic-survey-of-prompting-methods-in-natural-language-processingliu2021pretrain"},"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",(0,r.kt)("sup",{parentName:"h4",id:"fnref-55"},(0,r.kt)("a",{parentName:"sup",href:"#fn-55",className:"footnote-ref"},"55"))),(0,r.kt)("h4",{id:"promptpapersning2022papers"},"PromptPapers",(0,r.kt)("sup",{parentName:"h4",id:"fnref-56"},(0,r.kt)("a",{parentName:"sup",href:"#fn-56",className:"footnote-ref"},"56"))),(0,r.kt)("h2",{id:"miscl"},"Miscl"),(0,r.kt)("h4",{id:"learning-to-perform-complex-tasks-through-compositional-fine-tuning-of-language-modelsbursztyn2022learning"},"Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-57"},(0,r.kt)("a",{parentName:"sup",href:"#fn-57",className:"footnote-ref"},"57"))),(0,r.kt)("h4",{id:"super-naturalinstructions-generalization-via-declarative-instructions-on-1600-nlp-taskswang2022supernaturalinstructions"},"Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks",(0,r.kt)("sup",{parentName:"h4",id:"fnref-58"},(0,r.kt)("a",{parentName:"sup",href:"#fn-58",className:"footnote-ref"},"58"))),(0,r.kt)("h4",{id:"making-pre-trained-language-models-better-few-shot-learnersgao2021making"},"Making Pre-trained Language Models Better Few-shot Learners",(0,r.kt)("sup",{parentName:"h4",id:"fnref-59"},(0,r.kt)("a",{parentName:"sup",href:"#fn-59",className:"footnote-ref"},"59"))),(0,r.kt)("h4",{id:"grounding-with-search-resultslivin2022large"},"Grounding with search results",(0,r.kt)("sup",{parentName:"h4",id:"fnref-60"},(0,r.kt)("a",{parentName:"sup",href:"#fn-60",className:"footnote-ref"},"60"))),(0,r.kt)("h4",{id:"how-to-prompt-opportunities-and-challenges-of-zero--and-few-shot-learning-for-human-ai-interaction-in-creative-applications-of-generative-modelsdang2022prompt"},"How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-61"},(0,r.kt)("a",{parentName:"sup",href:"#fn-61",className:"footnote-ref"},"61"))),(0,r.kt)("h4",{id:"on-measuring-social-biases-in-prompt-based-multi-task-learningakyrek2022measuring"},"On Measuring Social Biases in Prompt-Based Multi-Task Learning",(0,r.kt)("sup",{parentName:"h4",id:"fnref-62"},(0,r.kt)("a",{parentName:"sup",href:"#fn-62",className:"footnote-ref"},"62"))),(0,r.kt)("h4",{id:"plot-writing-from-pre-trained-language-modelsjin2022plot-"},"Plot Writing From Pre-Trained Language Models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-63"},(0,r.kt)("a",{parentName:"sup",href:"#fn-63",className:"footnote-ref"},"63"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"examples2022examples"},"Examples",(0,r.kt)("sup",{parentName:"h4",id:"fnref-64"},(0,r.kt)("a",{parentName:"sup",href:"#fn-64",className:"footnote-ref"},"64"))),(0,r.kt)("h4",{id:"wordcraftyuan2022wordcraft"},"Wordcraft",(0,r.kt)("sup",{parentName:"h4",id:"fnref-65"},(0,r.kt)("a",{parentName:"sup",href:"#fn-65",className:"footnote-ref"},"65"))),(0,r.kt)("h4",{id:"painpointsfadnavis2022pain"},"PainPoints",(0,r.kt)("sup",{parentName:"h4",id:"fnref-66"},(0,r.kt)("a",{parentName:"sup",href:"#fn-66",className:"footnote-ref"},"66"))),(0,r.kt)("h4",{id:"self-instruct-aligning-language-model-with-self-generated-instructionswang2022selfinstruct"},"Self-Instruct: Aligning Language Model with Self Generated Instructions",(0,r.kt)("sup",{parentName:"h4",id:"fnref-67"},(0,r.kt)("a",{parentName:"sup",href:"#fn-67",className:"footnote-ref"},"67"))),(0,r.kt)("h4",{id:"from-images-to-textual-prompts-zero-shot-vqa-with-frozen-large-language-modelsguo2022images"},"From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-68"},(0,r.kt)("a",{parentName:"sup",href:"#fn-68",className:"footnote-ref"},"68"))),(0,r.kt)("div",{className:"footnotes"},(0,r.kt)("hr",{parentName:"div"}),(0,r.kt)("ol",{parentName:"div"},(0,r.kt)("li",{parentName:"ol",id:"fn-1"},"Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., & Zhou, D. (2022). Chain of Thought Prompting Elicits Reasoning in Large Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-1",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-2"},"Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large Language Models are Zero-Shot Reasoners.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-2",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-3"},"Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., & Zhou, D. (2022). Self-Consistency Improves Chain of Thought Reasoning in Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-3",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-4"},"Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., & Chen, W. (2021). What Makes Good In-Context Examples for GPT-3?\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-4",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-5"},"Liu, J., Liu, A., Lu, X., Welleck, S., West, P., Bras, R. L., Choi, Y., & Hajishirzi, H. (2021). Generated Knowledge Prompting for Commonsense Reasoning.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-5",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-6"},"Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., & Zettlemoyer, L. (2022). Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-6",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-7"},"Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., & Odena, A. (2021). Show Your Work: Scratchpads for Intermediate Computation with Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-7",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-8"},"Jung, J., Qin, L., Welleck, S., Brahman, F., Bhagavatula, C., Bras, R. L., & Choi, Y. (2022). Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-8",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-9"},"Zelikman, E., Wu, Y., Mu, J., & Goodman, N. D. (2022). STaR: Bootstrapping Reasoning With Reasoning.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-9",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-10"},"Zhou, D., Sch\xe4rli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q., & Chi, E. (2022). Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-10",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-11"},"Ye, X., & Durrett, G. (2022). The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-11",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-12"},"Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber, J., & Wang, L. (2022). Prompting GPT-3 To Be Reliable.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-12",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-13"},"Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., & Chen, W. (2022). On the Advance of Making Language Models Better Reasoners.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-13",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-14"},"Zhao, T. Z., Wallace, E., Feng, S., Klein, D., & Singh, S. (2021). Calibrate Before Use: Improving Few-Shot Performance of Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-14",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-15"},"Mitchell, E., Noh, J. J., Li, S., Armstrong, W. S., Agarwal, A., Liu, P., Finn, C., & Manning, C. D. (2022). Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-15",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-16"},"Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., & Singh, S. (2020). AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). https://doi.org/10.18653/v1/2020.emnlp-main.346\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-16",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-17"},"Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., & Ba, J. (2022). Large Language Models Are Human-Level Prompt Engineers.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-17",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-18"},"Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., \u2026 Amodei, D. (2020). Language Models are Few-Shot Learners.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-18",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-19"},"Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., & Lowe, R. (2022). Training language models to follow instructions with human feedback.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-19",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-20"},"Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., \u2026 Fiedel, N. (2022). PaLM: Scaling Language Modeling with Pathways.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-20",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-21"},"Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili\u0107, S., Hesslow, D., Castagn\xe9, R., Luccioni, A. S., Yvon, F., Gall\xe9, M., Tow, J., Rush, A. M., Biderman, S., Webson, A., Ammanamanchi, P. S., Wang, T., Sagot, B., Muennighoff, N., del Moral, A. V., \u2026 Wolf, T. (2022). BLOOM: A 176B-Parameter Open-Access Multilingual Language Model.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-21",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-22"},"Yong, Z.-X., Schoelkopf, H., Muennighoff, N., Aji, A. F., Adelani, D. I., Almubarak, K., Bari, M. S., Sutawika, L., Kasai, J., Baruwa, A., Winata, G. I., Biderman, S., Radev, D., & Nikoulina, V. (2022). BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-22",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-23"},"Lieber, O., Sharir, O., Lentz, B., & Shoham, Y. (2021). Jurassic-1: Technical Details and Evaluation, White paper, AI21 Labs, 2021. URL: Https://Uploads-Ssl. Webflow. Com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_ Tech_paper. Pdf.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-23",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-24"},"Wang, B., & Komatsuzaki, A. (2021). GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax. https://github.com/kingoflolz/mesh-transformer-jax\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-24",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-25"},"Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv Preprint arXiv:1907.11692.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-25",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-26"},"Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2021). High-Resolution Image Synthesis with Latent Diffusion Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-26",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-27"},"Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical Text-Conditional Image Generation with CLIP Latents.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-27",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-28"},"Lester, B., Al-Rfou, R., & Constant, N. (2021). The Power of Scale for Parameter-Efficient Prompt Tuning.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-28",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-29"},"Khashabi, D., Lyu, S., Min, S., Qin, L., Richardson, K., Welleck, S., Hajishirzi, H., Khot, T., Sabharwal, A., Singh, S., & Choi, Y. (2021). Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-29",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-30"},"Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., & Schulman, J. (2021). Training Verifiers to Solve Math Word Problems.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-30",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-31"},"Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., & Manning, C. D. (2018). HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-31",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-32"},"Thorne, J., Vlachos, A., Christodoulopoulos, C., & Mittal, A. (2018). FEVER: a large-scale dataset for Fact Extraction and VERification.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-32",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-33"},"Parrish, A., Chen, A., Nangia, N., Padmakumar, V., Phang, J., Thompson, J., Htut, P. M., & Bowman, S. R. (2021). BBQ: A Hand-Built Bias Benchmark for Question Answering.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-33",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-34"},"Oppenlaender, J. (2022). A Taxonomy of Prompt Modifiers for Text-To-Image Generation.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-34",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-35"},"Wang, Z. J., Montoya, E., Munechika, D., Yang, H., Hoover, B., & Chau, D. H. (2022). DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-35",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-36"},"Parsons, G. (2022). The DALLE 2 Prompt Book. https://dallery.gallery/the-dalle-2-prompt-book/\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-36",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-37"},"Oppenlaender, J. (2022). Prompt Engineering for Text-Based Generative Art.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-37",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-38"},"Strobelt, H., Webson, A., Sanh, V., Hoover, B., Beyer, J., Pfister, H., & Rush, A. M. (2022). Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models. arXiv. https://doi.org/10.48550/ARXIV.2208.07852\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-38",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-39"},"Bach, S. H., Sanh, V., Yong, Z.-X., Webson, A., Raffel, C., Nayak, N. V., Sharma, A., Kim, T., Bari, M. S., Fevry, T., Alyafeai, Z., Dey, M., Santilli, A., Sun, Z., Ben-David, S., Xu, C., Chhablani, G., Wang, H., Fries, J. A., \u2026 Rush, A. M. (2022). PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-39",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-40"},"Wu, T., Jiang, E., Donsbach, A., Gray, J., Molina, A., Terry, M., & Cai, C. J. (2022). PromptChainer: Chaining Large Language Model Prompts through Visual Programming.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-40",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-41"},"Ding, N., Hu, S., Zhao, W., Chen, Y., Liu, Z., Zheng, H.-T., & Sun, M. (2021). OpenPrompt: An Open-source Framework for Prompt-learning. arXiv Preprint arXiv:2111.01998.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-41",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-42"},"Jiang, E., Olson, K., Toh, E., Molina, A., Donsbach, A., Terry, M., & Cai, C. J. (2022). PromptMaker: Prompt-Based Prototyping with Large&nbsp;Language&nbsp;Models. Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3491101.3503564\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-42",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-43"},"Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R. G., Wu, Y., Michalewski, H., Saurous, R. A., Sohl-dickstein, J., Murphy, K., & Sutton, C. (2022). Language Model Cascades.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-43",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-44"},"Karpas, E., Abend, O., Belinkov, Y., Lenz, B., Lieber, O., Ratner, N., Shoham, Y., Bata, H., Levine, Y., Leyton-Brown, K., Muhlgay, D., Rozen, N., Schwartz, E., Shachaf, G., Shalev-Shwartz, S., Shashua, A., & Tenenholtz, M. (2022). MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-44",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-45"},"Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2022). ReAct: Synergizing Reasoning and Acting in Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-45",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-46"},"Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., & Neubig, G. (2022). PAL: Program-aided Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-46",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-47"},"Liu, V., & Chilton, L. B. (2022). Design Guidelines for Prompt Engineering Text-to-Image Generative Models. Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3491102.3501825\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-47",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-48"},"Crothers, E., Japkowicz, N., & Viktor, H. (2022). Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-48",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-49"},"Branch, H. J., Cefalu, J. R., McHugh, J., Hujer, L., Bahl, A., del Castillo Iglesias, D., Heichman, R., & Darwishi, R. (2022). Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-49",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-50"},"Willison, S. (2022). Prompt injection attacks against GPT-3. https://simonwillison.net/2022/Sep/12/prompt-injection/\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-50",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-51"},"Goodside, R. (2022). Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions. https://twitter.com/goodside/status/1569128808308957185\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-51",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-52"},"Chase, H. (2022). adversarial-prompts. https://github.com/hwchase17/adversarial-prompts\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-52",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-53"},"Goodside, R. (2022). GPT-3 Prompt Injection Defenses. https://twitter.com/goodside/status/1578278974526222336?s=20&t=3UMZB7ntYhwAk3QLpKMAbw\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-53",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-54"},"Mark, C. (2022). Talking to machines: prompt engineering & injection. https://artifact-research.com/artificial-intelligence/talking-to-machines-prompt-engineering-injection/\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-54",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-55"},"Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2022). Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. ACM Computing Surveys. https://doi.org/10.1145/3560815\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-55",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-56"},"PromptPapers. (2022). https://github.com/thunlp/PromptPapers\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-56",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-57"},"Bursztyn, V. S., Demeter, D., Downey, D., & Birnbaum, L. (2022). Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-57",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-58"},"Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran, A. S., Naik, A., Stap, D., Pathak, E., Karamanolakis, G., Lai, H. G., Purohit, I., Mondal, I., Anderson, J., Kuznia, K., Doshi, K., Patel, M., \u2026 Khashabi, D. (2022). Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-58",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-59"},"Gao, T., Fisch, A., & Chen, D. (2021). Making Pre-trained Language Models Better Few-shot Learners. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). https://doi.org/10.18653/v1/2021.acl-long.295\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-59",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-60"},"Li\xe9vin, V., Hother, C. E., & Winther, O. (2022). Can large language models reason about medical questions?\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-60",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-61"},"Dang, H., Mecke, L., Lehmann, F., Goller, S., & Buschek, D. (2022). How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-61",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-62"},"Aky\xfcrek, A. F., Paik, S., Kocyigit, M. Y., Akbiyik, S., Runyun, \u015e. L., & Wijaya, D. (2022). On Measuring Social Biases in Prompt-Based Multi-Task Learning.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-62",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-63"},"Jin, Y., Kadam, V., & Wanvarie, D. (2022). Plot Writing From Pre-Trained Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-63",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-64"},"Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., & Chen, W. (2022). What Makes Good In-Context Examples for GPT-3? Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. https://doi.org/10.18653/v1/2022.deelio-1.10\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-64",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-65"},"Yuan, A., Coenen, A., Reif, E., & Ippolito, D. (2022). Wordcraft: Story Writing With Large Language Models. 27th International Conference on Intelligent User Interfaces, 841\u2013852.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-65",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-66"},"Fadnavis, S., Dhurandhar, A., Norel, R., Reinen, J. M., Agurto, C., Secchettin, E., Schweiger, V., Perini, G., & Cecchi, G. (2022). PainPoints: A Framework for Language-based Detection of Chronic Pain and Expert-Collaborative Text-Summarization. arXiv Preprint arXiv:2209.09814.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-66",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-67"},"Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., & Hajishirzi, H. (2022). Self-Instruct: Aligning Language Model with Self Generated Instructions.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-67",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-68"},"Guo, J., Li, J., Li, D., Tiong, A. M. H., Li, B., Tao, D., & Hoi, S. C. H. (2022). From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-68",className:"footnote-backref"},"\u21a9")))))}m.isMDXComponent=!0}}]);