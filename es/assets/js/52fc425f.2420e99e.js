"use strict";(self.webpackChunkpromptgineering=self.webpackChunkpromptgineering||[]).push([[7006],{51321:(t,e,i)=>{i.r(e),i.d(e,{assets:()=>c,contentTitle:()=>p,default:()=>g,frontMatter:()=>a,metadata:()=>s,toc:()=>l});var o=i(87462),n=(i(67294),i(3905)),r=i(39145);const a={sidebar_position:0},p="\ud83d\udfe2 Introduction",s={unversionedId:"prompt_hacking/intro",id:"prompt_hacking/intro",title:"\ud83d\udfe2 Introduction",description:"%%LLMs|LLM%% are vulnerable to a form of hacking called prompt hacking. Prompt hacking is very different from regular hacking, which involves exploiting vulnerabilities in code. Prompt hacking is usually more similar to social engineering, in the sense that it involves using words to trick the %%LLM|LLM%% into doing something it would not normally do.",source:"@site/docs/prompt_hacking/intro.md",sourceDirName:"prompt_hacking",slug:"/prompt_hacking/intro",permalink:"/es/docs/prompt_hacking/intro",draft:!1,editUrl:"https://github.com/trigaten/promptgineering/tree/v1.2.3/docs/prompt_hacking/intro.md",tags:[],version:"current",sidebarPosition:0,frontMatter:{sidebar_position:0},sidebar:"tutorialSidebar",previous:{title:"\ud83d\udd13 Prompt Hacking",permalink:"/es/docs/category/-prompt-hacking"},next:{title:"\ud83d\udfe2 Inyecci\xf3n de Prompt",permalink:"/es/docs/prompt_hacking/injection"}},c={},l=[],d={toc:l},m="wrapper";function g(t){let{components:e,...i}=t;return(0,n.kt)(m,(0,o.Z)({},d,i,{components:e,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"-introduction"},"\ud83d\udfe2 Introduction"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",id:"LLM_0_0_1682876952793","data-tooltip-html":"Large Language Model. A model that is trained to predict the next word in a sentence.","data-tooltip-place":"top"},"LLMs"),(0,n.kt)(r.u,{anchorId:"LLM_0_0_1682876952793",clickable:!0,mdxType:"Tooltip"})," are vulnerable to a form of hacking called ",(0,n.kt)("em",{parentName:"p"},"prompt hacking"),". Prompt hacking is very different from regular hacking, which involves exploiting vulnerabilities in code. Prompt hacking is usually more similar to social engineering, in the sense that it involves using words to trick the ",(0,n.kt)("a",{parentName:"p",id:"LLM_4_225_1682876952793","data-tooltip-html":"Large Language Model. A model that is trained to predict the next word in a sentence.","data-tooltip-place":"top"},"LLM"),(0,n.kt)(r.u,{anchorId:"LLM_4_225_1682876952793",clickable:!0,mdxType:"Tooltip"})," into doing something it would not normally do."),(0,n.kt)("p",null,"In this chapter, we will cover three distinct types of prompt hacking: prompt injection, prompt leaking, and jailbreaking. We will also discuss specific offensive techniques as well as defensive techniques."))}g.isMDXComponent=!0}}]);