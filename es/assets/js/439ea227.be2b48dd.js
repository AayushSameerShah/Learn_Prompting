"use strict";(self.webpackChunkpromptgineering=self.webpackChunkpromptgineering||[]).push([[5470],{3905:(e,a,t)=>{t.d(a,{Zo:()=>c,kt:()=>f});var o=t(67294);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function r(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);a&&(o=o.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,o)}return t}function i(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?r(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,o,n=function(e,a){if(null==e)return{};var t,o,n={},r=Object.keys(e);for(o=0;o<r.length;o++)t=r[o],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)t=r[o],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var l=o.createContext({}),p=function(e){var a=o.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):i(i({},a),e)),t},c=function(e){var a=p(e.components);return o.createElement(l.Provider,{value:a},e.children)},d="mdxType",m={inlineCode:"code",wrapper:function(e){var a=e.children;return o.createElement(o.Fragment,{},a)}},u=o.forwardRef((function(e,a){var t=e.components,n=e.mdxType,r=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),d=p(t),u=n,f=d["".concat(l,".").concat(u)]||d[u]||m[u]||r;return t?o.createElement(f,i(i({ref:a},c),{},{components:t})):o.createElement(f,i({ref:a},c))}));function f(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var r=t.length,i=new Array(r);i[0]=u;var s={};for(var l in a)hasOwnProperty.call(a,l)&&(s[l]=a[l]);s.originalType=e,s[d]="string"==typeof e?e:n,i[1]=s;for(var p=2;p<r;p++)i[p]=t[p];return o.createElement.apply(null,i)}return o.createElement.apply(null,t)}u.displayName="MDXCreateElement"},28463:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>c,contentTitle:()=>l,default:()=>f,frontMatter:()=>s,metadata:()=>p,toc:()=>d});var o=t(87462),n=(t(67294),t(3905));const r=t.p+"assets/images/zero_shot-1af9e1cb88412f9fdefa3b07b67c4193.png",i=t.p+"assets/images/zero_shot_example-89065990663d4ef044011844ff77f9af.png",s={sidebar_position:4},l="\ud83d\udfe2 Zero Shot Chain of Thought",p={unversionedId:"intermediate/zero_shot_cot",id:"intermediate/zero_shot_cot",title:"\ud83d\udfe2 Zero Shot Chain of Thought",description:'La cadena de pensamiento sin entrenamiento previo (Zero Shot Chain of Thought, Zero-shot-CoT) (@kojima2022large) es una continuaci\xf3n de la iniciativa de cadena de pensamiento (CoT prompting) (@wei2022chain), que introduce una simple frase inicial sin entrenamiento previo. Encuentran que al agregar las palabras "Pensemos paso a paso." al final de una pregunta, las LM (lenguajes modelo) son capaces de generar una cadena de pensamiento que responde la pregunta. A partir de esta cadena de pensamiento, pueden extraer respuestas m\xe1s precisas.',source:"@site/i18n/es/docusaurus-plugin-content-docs/current/intermediate/zero_shot_cot.md",sourceDirName:"intermediate",slug:"/intermediate/zero_shot_cot",permalink:"/es/docs/intermediate/zero_shot_cot",draft:!1,editUrl:"https://github.com/trigaten/promptgineering/tree/v1.2.2/docs/intermediate/zero_shot_cot.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"\ud83d\udfe2 Cadena de pensamiento",permalink:"/es/docs/intermediate/chain_of_thought"},next:{title:"\ud83d\udfe1 Autoconsistencia",permalink:"/es/docs/intermediate/self_consistency"}},c={},d=[{value:"Ejemplo",id:"ejemplo",level:2},{value:"Incorrecta",id:"incorrecta",level:4},{value:"Correcta",id:"correcta",level:4},{value:"Resultados",id:"resultados",level:2},{value:"Ablaciones de Inter\xe9s",id:"ablaciones-de-inter\xe9s",level:2},{value:"Notas",id:"notas",level:2}],m={toc:d},u="wrapper";function f(e){let{components:a,...t}=e;return(0,n.kt)(u,(0,o.Z)({},m,t,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"-zero-shot-chain-of-thought"},"\ud83d\udfe2 Zero Shot Chain of Thought"),(0,n.kt)("p",null,"La cadena de pensamiento sin entrenamiento previo (Zero Shot Chain of Thought, Zero-shot-CoT)",(0,n.kt)("sup",{parentName:"p",id:"fnref-1"},(0,n.kt)("a",{parentName:"sup",href:"#fn-1",className:"footnote-ref"},"1"))," es una continuaci\xf3n de la iniciativa de cadena de pensamiento (CoT prompting)",(0,n.kt)("sup",{parentName:"p",id:"fnref-2"},(0,n.kt)("a",{parentName:"sup",href:"#fn-2",className:"footnote-ref"},"2")),', que introduce una simple frase inicial sin entrenamiento previo. Encuentran que al agregar las palabras "',(0,n.kt)("strong",{parentName:"p"},"Pensemos paso a paso."),'" al final de una pregunta, las LM (lenguajes modelo) son capaces de generar una cadena de pensamiento que responde la pregunta. A partir de esta cadena de pensamiento, pueden extraer respuestas m\xe1s precisas.'),(0,n.kt)("div",{style:{textAlign:"center"}},(0,n.kt)("img",{src:r,style:{width:"500px"}})),(0,n.kt)("div",{style:{textAlign:"center"}},"Zero Shot CoT (Kojima et al.)"),(0,n.kt)("p",null,"T\xe9cnicamente, el proceso completo de Zero-shot-CoT implica dos frases iniciales/completados separados. En la siguiente imagen, la burbuja superior de la izquierda genera una cadena de pensamiento, mientras que la burbuja superior de la derecha toma la salida del primer completado (incluyendo el propio completado) y extrae la respuesta de la cadena de pensamiento. Este segundo completado es un completado ",(0,n.kt)("em",{parentName:"p"},"auto-aumentado"),"."),(0,n.kt)("div",{style:{textAlign:"center"}},(0,n.kt)("img",{src:i,style:{width:"500px"}})),(0,n.kt)("div",{style:{textAlign:"center"}},"Full Zero Shot CoT Process (Kojima et al.)"),(0,n.kt)("h2",{id:"ejemplo"},"Ejemplo"),(0,n.kt)("p",null,'Aqu\xed tienes algunas demostraciones (que solo realizan extracci\xf3n de razonamiento). Esta primera demostraci\xf3n muestra c\xf3mo GPT-3 (davinci-003) no responde correctamente a una simple pregunta matem\xe1tica, mientras que la segunda demostraci\xf3n utiliza una frase inicial de Zero-shot-CoT y resuelve con \xe9xito el problema. Si\xe9ntete libre de ingresar tu clave API de OpenAI (haz clic en "Generar") y jugar con los ejemplos. Observa lo mucho m\xe1s simple que es la frase inicial de Zero-shot-CoT en comparaci\xf3n con la de CoT.'),(0,n.kt)("h4",{id:"incorrecta"},"Incorrecta"),(0,n.kt)("div",{"trydyno-embed":"","openai-model":"text-davinci-003","initial-prompt":"Si John tiene 5 peras, luego come 2, y compra 5 m\xe1s, luego le da 3 a su amigo, \xbfcu\xe1ntas peras tiene?","initial-response":"John tiene 8 peras.","max-tokens":"256","box-rows":"3","model-temp":"0.7","top-p":"1"}),(0,n.kt)("h4",{id:"correcta"},"Correcta"),(0,n.kt)("div",{"trydyno-embed":"","openai-model":"text-davinci-003","initial-prompt":"Si John tiene 5 peras, luego come 2, y compra 5 m\xe1s, luego le da 3 a su amigo, \xbfcu\xe1ntas peras tiene?\\n\\nPensemos paso a paso.","initial-response":"John comienza con 5 peras. Come 2 peras, lo que lo deja con 3 peras. Compra 5 peras m\xe1s, lo que le da un total de 8 peras. Le da 3 peras a su amigo, lo que lo deja con solo 5 peras.","max-tokens":"256","box-rows":"5","model-temp":"0.7","top-p":"1"}),(0,n.kt)("h2",{id:"resultados"},"Resultados"),(0,n.kt)("p",null,"Zero-shot-CoT tambi\xe9n result\xf3 efectivo para mejorar los resultados en tareas de aritm\xe9tica, razonamiento com\xfan y simb\xf3lico. Sin embargo, como era de esperar, generalmente no fue tan efectivo como CoT prompting. Un caso de uso importante para Zero-shot-CoT es cuando es dif\xedcil obtener ejemplos de pocos disparos para el CoT prompting."),(0,n.kt)("h2",{id:"ablaciones-de-inter\xe9s"},"Ablaciones de Inter\xe9s"),(0,n.kt)("p",null,'Kojima y su equipo experimentaron con una serie de diferentes comandos Zero-shot-CoT (por ejemplo, "Resolvamos este problema dividi\xe9ndolo en pasos" o "Pensemos en esto l\xf3gicamente"), pero descubrieron que "Pensemos paso a paso" es el m\xe1s efectivo para sus tareas elegidas.'),(0,n.kt)("h2",{id:"notas"},"Notas"),(0,n.kt)("p",null,"El paso de extracci\xf3n a menudo debe ser espec\xedfico de la tarea, lo que hace que Zero-Shot-CoT sea menos generalizable de lo que parece al principio."),(0,n.kt)("p",null,"De manera anecdotica, he encontrado que los comandos de estilo Zero-shot-CoT a veces son efectivos para mejorar la longitud de las completaciones para tareas generativas. Por ejemplo, considere el comando est\xe1ndar ",(0,n.kt)("inlineCode",{parentName:"p"},"Write a story about a frog and a mushroom who become friends."),"\nAgregar las palabras ",(0,n.kt)("inlineCode",{parentName:"p"},"Let's think step by step.")," al final de este comando lleva a una finalizaci\xf3n mucho m\xe1s larga."),(0,n.kt)("div",{className:"footnotes"},(0,n.kt)("hr",{parentName:"div"}),(0,n.kt)("ol",{parentName:"div"},(0,n.kt)("li",{parentName:"ol",id:"fn-1"},"Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large Language Models are Zero-Shot Reasoners.\n",(0,n.kt)("a",{parentName:"li",href:"#fnref-1",className:"footnote-backref"},"\u21a9")),(0,n.kt)("li",{parentName:"ol",id:"fn-2"},"Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., & Zhou, D. (2022). Chain of Thought Prompting Elicits Reasoning in Large Language Models.\n",(0,n.kt)("a",{parentName:"li",href:"#fnref-2",className:"footnote-backref"},"\u21a9")))))}f.isMDXComponent=!0}}]);