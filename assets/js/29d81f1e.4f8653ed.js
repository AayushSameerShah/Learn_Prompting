"use strict";(self.webpackChunkpromptgineering=self.webpackChunkpromptgineering||[]).push([[930],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>h});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),m=p(n),h=r,g=m["".concat(l,".").concat(h)]||m[h]||u[h]||i;return n?a.createElement(g,o(o({ref:t},c),{},{components:n})):a.createElement(g,o({ref:t},c))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var p=2;p<i;p++)o[p]=n[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},9621:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var a=n(7462),r=(n(7294),n(3905));const i={sidebar_position:0},o="Prompting",s={unversionedId:"basics/prompting",id:"basics/prompting",title:"Prompting",description:"Before we get to Prompt Engineering, what is prompting? It can be defined as",source:"@site/docs/basics/prompting.md",sourceDirName:"basics",slug:"/basics/prompting",permalink:"/docs/basics/prompting",draft:!1,editUrl:"https://github.com/trigaten/promptgineering/tree/v0.0.2/docs/basics/prompting.md",tags:[],version:"current",sidebarPosition:0,frontMatter:{sidebar_position:0},sidebar:"tutorialSidebar",previous:{title:"\ud83d\ude03 Basics",permalink:"/docs/category/-basics"},next:{title:"More on Prompting",permalink:"/docs/basics/more_on_prompting"}},l={},p=[{value:"1) Sentiment Analysis",id:"1-sentiment-analysis",level:4},{value:"2) Math Word Problem Solving",id:"2-math-word-problem-solving",level:4}],c={toc:p};function u(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,a.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"prompting"},"Prompting"),(0,r.kt)("p",null,'Before we get to Prompt Engineering, what is prompting? It can be defined as\n"converting tasks into a language model format"',(0,r.kt)("sup",{parentName:"p",id:"fnref-1"},(0,r.kt)("a",{parentName:"sup",href:"#fn-1",className:"footnote-ref"},"1")),". Slightly less\nformally, it is the process of taking a task and converting it into a question that\ncan be answered by a language model. Here are two examples of this:"),(0,r.kt)("h4",{id:"1-sentiment-analysis"},"1) Sentiment Analysis"),(0,r.kt)("p",null,'If you are performing sentiment analysis on Tweets with the binary labels "positive" and "negative",\nyou can convert this into a language model format by posing the question "Is this tweet positive or negative?"'),(0,r.kt)("p",null,'For example, the sentence "I love this movie!" is positive, while the sentence "I hate this movie!" is negative.\nYour full prompt could look like this:'),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'Tweet: "What a beautiful day!"\n\nIs this tweet positive or negative?\n')),(0,r.kt)("h4",{id:"2-math-word-problem-solving"},"2) Math Word Problem Solving"),(0,r.kt)("p",null,'If you have a dataset of mathematical equations that you would like a language model to solve,\nyou can convert this into a language model format by posing the question "What is EQUATION"'),(0,r.kt)("p",null,"Your full prompt could look like this:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"What is 100*100?\n")),(0,r.kt)("h1",{id:"intro-to-prompt-engineering"},"Intro to Prompt Engineering"),(0,r.kt)("p",null,"For this last prompt, GPT-3 (text-davinci-002) sometimes answers 1,000 (incorrect). This is where\nprompt engineering comes in. If, instead of asking ",(0,r.kt)("inlineCode",{parentName:"p"},'"What is 100*100?"'),", we ask\n",(0,r.kt)("inlineCode",{parentName:"p"},'"What is 100*100? Make sure your answer has the correct number of 0s:"'),", GPT-3 will\nanswer 10,000 (correct). Why is this the case? Why is the additional specification\nof the number of zeros necessary for the AI to get the right answer? How can we create\nprompts which yield optimal results on our task? This last question, in particular,\nis the focus of the field of Prompt Engineering, as well as this course."),(0,r.kt)("h1",{id:"vocabulary"},"Vocabulary"),(0,r.kt)("p",null,"We will cover basic vocabulary here, but frequently assume basic ML/NLP (natural language processing) knowledge. "),(0,r.kt)("p",null,"Although familiarity with LLMs and MLMs is mostly assumed,\nwe provide a short introduction to each concept here:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Large Language Models (LLMs), Pretrained Language Models (PLMs)",(0,r.kt)("sup",{parentName:"li",id:"fnref-2"},(0,r.kt)("a",{parentName:"sup",href:"#fn-2",className:"footnote-ref"},"2")),", Language Models (LMs), and foundation models")),(0,r.kt)("p",null,"These terms all refer to the same thing: large neural networks, which have usually been trained\non a huge amount of text."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Masked Language Models (MLMs)")),(0,r.kt)("p",null,"MLMs are a type of NLP model, which have a special token, usually ",(0,r.kt)("inlineCode",{parentName:"p"},"[MASK]"),', which is\nreplaced with a word from the vocabulary. The model then predicts the word that\nwas masked. For example, if the sentence is "The dog is ',"[MASK]",' the cat", the model\nwill predict "chasing" with high probability.'),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Sentiment Analysis")),(0,r.kt)("p",null,"Sentiment analysis is the task of classifying text into positive, negative, or other sentiments."),(0,r.kt)("div",{className:"footnotes"},(0,r.kt)("hr",{parentName:"div"}),(0,r.kt)("ol",{parentName:"div"},(0,r.kt)("li",{parentName:"ol",id:"fn-1"},"Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., & Singh, S. (2020). AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). https://doi.org/10.18653/v1/2020.emnlp-main.346\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-1",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-2"},"Branch, H. J., Cefalu, J. R., McHugh, J., Hujer, L., Bahl, A., del Castillo Iglesias, D., Heichman, R., & Darwishi, R. (2022). Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-2",className:"footnote-backref"},"\u21a9")))))}u.isMDXComponent=!0}}]);